---
layout:     post                    # 使用的布局（不需要改）
title:      特征降维   				# 标题 		  
subtitle:   特征选择、PCA 	# 副标题
date:       2021-02-24              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 特征工程
---

# 一、简介
## 1. 定义
降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyy0nyxirj30om0go7bi.jpg)

## 2. 降维的两种方式
- 特征选择
- 主成分分析(可以理解一种特征提取的方式)

# 二、特征选择

## 1. 定义
数据中包含冗余或无关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征

## 2. 方法
- **Filter(过滤式)**:主要探究特征本身特点、特征与特征和目标值之间关联 
	- **方差选择法**:低方差特征过滤
	- **相关系数**:
		- 皮尔逊相关系数
		- 斯皮尔曼相关系数
- **Embedded (嵌入式)**:算法自动选择特征(特征与目标值之间的关联) 
	- **决策树**:信息熵、信息增益
	- **正则化**:L1、L2
	- **深度学习**:卷积等

## 3. 低方差特征过滤
- 删除低方差的一些特征；
	- 特征方差小:某个特征大多样本的值比较相近
	- 特征方差大:某个特征很多样本的值都有差别

### 3.1 API
- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
	- 删除所有低方差特征
	- Variance.fit_transform(X)
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值:训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征

### 3.2 案例
```python
def variance_demo(): 
	"""
	删除低方差特征——特征选择
	:return: None
	"""
	data = pd.read_csv("factor_returns.csv") print(data)
	# 1、实例化一个转换器类

	transfer = VarianceThreshold(threshold=1)
	# 2、调用fit_transform

	data = transfer.fit_transform(data.iloc[:, 1:10]) print("删除低方差特征的结果:\n", data) print("形状:\n", data.shape)
	return None
```

## 4. 相关系数
### 4.1 皮尔逊相关系数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyy83ye3bj30na04n75h.jpg)

#### 4.1.1 特点
- 相关系数的值介于–1与+1之间，即–1≤ r ≤+1。其性质如下:
	- 当r>0时，表示两变量正相关，r<0时，两变量为负相关
	- 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系 
	- 当0<|r|<1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切;|r|越接近于0，表示两变量的线性相关越弱
	- 一般可按三级划分:|r|<0.4为低度相关;0.4≤|r|<0.7为显著性相关;0.7≤|r|<1为高度线性相关

#### 4.1.2 api
- from scipy.stats import pearsonr 
	- x : (N,) array_like
	- y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)

#### 4.1.3 案例
```python
from scipy.stats import pearsonr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5] 
pearsonr(x1, x2)
```

### 4.2 斯皮尔曼相关系数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyybfmnqbj30lr03udg7.jpg)

#### 4.2.1 特点
- 斯皮尔曼相关系数表明 X (自变量) 和 Y (因变量)的相关方向。如果当X增加时， Y趋向于增加, 斯皮尔曼相关系数则为正
- 与之前的皮尔逊相关系数大小性质一样，取值 [-1, 1]之间 
- 斯皮尔曼相关系数比皮尔逊相关系数应用更加广泛

#### 4.2.2 api
- from scipy.stats import spearmanr

#### 4.2.3 案例
```python
from scipy.stats import spearmanr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5] 
spearmanr(x1, x2)
```

# 三、主成分分析(PCA)