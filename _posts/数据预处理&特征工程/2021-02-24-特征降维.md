---
layout:     post                    # 使用的布局（不需要改）
title:      特征降维   				# 标题 		  
subtitle:   特征选择、PCA 	# 副标题
date:       2020-02-24              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 特征工程
---

# 一、简介
## 1. 定义
降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyy0nyxirj30om0go7bi.jpg)

## 2. 降维的方式
- 特征选择
	- Filter(过滤式)
		- 方差选择法
		- 相关系数
			- 皮尔逊相关系数
			- 斯皮尔曼相关系数
	- Embedded (嵌入式)
		- 决策树:信息熵、信息增益
		- 正则化:L1、L2
		- 深度学习:卷积等
- 线性降维：
	- PCA(可以理解一种特征提取的方式)
- 非线性降维：
	- 流形学习
		- ISOmap(Isometric Mapping)
		- LLE(Locally Linear Embedding) 
		- LE(Laplacian Eigenmaps)

# 二、维度灾难
![](https://tva1.sinaimg.cn/large/008eGmZEly1go0u6f8ojyj31h00j01e0.jpg)

## 1. 文献资料
[文献参考](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)

## 2. 有哪些灾难？
- 高维度带来数据稀疏，且分布在角落
- 样本数量指数增长


# 三、特征选择

## 1. 定义
数据中包含冗余或无关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征

## 2. 方法
- **Filter(过滤式)**:主要探究特征本身特点、特征与特征和目标值之间关联 
	- **方差选择法**:低方差特征过滤
	- **相关系数**:
		- 皮尔逊相关系数
		- 斯皮尔曼相关系数
- **Embedded (嵌入式)**:算法自动选择特征(特征与目标值之间的关联) 
	- **决策树**:信息熵、信息增益
	- **正则化**:L1、L2
	- **深度学习**:卷积等

## 3. 低方差特征过滤
- 删除低方差的一些特征；
	- 特征方差小:某个特征大多样本的值比较相近
	- 特征方差大:某个特征很多样本的值都有差别

### 3.1 API
- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
	- 删除所有低方差特征
	- Variance.fit_transform(X)
		- X:numpy array格式的数据[n_samples,n_features]
		- 返回值:训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征

### 3.2 案例
```python
def variance_demo(): 
	"""
	删除低方差特征——特征选择
	:return: None
	"""
	data = pd.read_csv("factor_returns.csv") print(data)
	# 1、实例化一个转换器类

	transfer = VarianceThreshold(threshold=1)
	# 2、调用fit_transform

	data = transfer.fit_transform(data.iloc[:, 1:10]) print("删除低方差特征的结果:\n", data) print("形状:\n", data.shape)
	return None
```

## 4. 相关系数
### 4.1 皮尔逊相关系数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyy83ye3bj30na04n75h.jpg)

#### 4.1.1 特点
- 相关系数的值介于–1与+1之间，即–1≤ r ≤+1。其性质如下:
	- 当r>0时，表示两变量正相关，r<0时，两变量为负相关
	- 当\|r\|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系 
	- 当0<\|r\|<1时，表示两变量存在一定程度的相关。且\|r\|越接近1，两变量间线性关系越密切;\|r\|越接近于0，表示两变量的线性相关越弱
	- 一般可按三级划分:\|r\|<0.4为低度相关;0.4≤\|r\|<0.7为显著性相关;0.7≤\|r\|<1为高度线性相关

#### 4.1.2 api
- from scipy.stats import pearsonr 
	- x : (N,) array_like
	- y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)

#### 4.1.3 案例
```python
from scipy.stats import pearsonr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5] 
pearsonr(x1, x2)
```

### 4.2 斯皮尔曼相关系数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyybfmnqbj30lr03udg7.jpg)

#### 4.2.1 特点
- 斯皮尔曼相关系数表明 X (自变量) 和 Y (因变量)的相关方向。如果当X增加时， Y趋向于增加, 斯皮尔曼相关系数则为正
- 与之前的皮尔逊相关系数大小性质一样，取值 [-1, 1]之间 
- 斯皮尔曼相关系数比皮尔逊相关系数应用更加广泛

#### 4.2.2 api
- from scipy.stats import spearmanr

#### 4.2.3 案例
```python
from scipy.stats import spearmanr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5] 
spearmanr(x1, x2)
```

# 四、主成分分析(PCA)

## 1. 什么是主成分分析
- 定义:高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
- 作用:是数据维数压缩，尽可能降低原数据的维数(复杂度)，损失少量信息。

## 2. 目标 
- 一个中心：
	- 原始特征空间的重构（相关->无关）
- 两个基本点：
	- 最大投影方差
	- 最小重构代价(误差平方和)

## 3. 推导过程
> 本质上就是求数据协方差矩阵的特征值和特征向量

![](https://tva1.sinaimg.cn/large/008eGmZEly1go0wextan5j30u00y9tnx.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1go0wexlz66j30u010y4a9.jpg)

注：上述证明是从最大投影方差角度，也可以从最小重构代价(误差平方和)来证明

## 4. PCA与SVD(奇异值分解)的联系
![](https://tva1.sinaimg.cn/large/008eGmZEly1go130spl4sj31as0nytct.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1go1310mefoj31dq0iijuo.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1go130zuzxhj31ds0i0juh.jpg)

## 5. API

- sklearn.decomposition.PCA(n_components=None)
	- 将数据分解为较低维数空间
	- n_components:
		- 小数:表示保留百分之多少的信息
		- 整数:减少到多少特征
	- PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features] 
	- 返回值:转换后指定维度的array

## 6. 代码
```python
from sklearn.decomposition import PCA

def pca_demo(): 
	"""
	对数据进行PCA降维
	:return: None
	"""
	data = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]
	# 1、实例化PCA, 小数——保留多少信息 

	transfer = PCA(n_components=0.9)
	# 2、调用fit_transform

	data1 = transfer.fit_transform(data)
	print("保留90%的信息，降维结果为:\n", data1)
	# 1、实例化PCA, 整数——指定降维到的维数 

	transfer2 = PCA(n_components=3)
	# 2、调用fit_transform

	data2 = transfer2.fit_transform(data) print("降维到3维的结果:\n", data2)
	return None
```
