---
layout:     post                    # 使用的布局（不需要改）
title:      深度神经网络-上		    # 标题 
subtitle:   定义、激活函数、常见损失	    # 副标题
date:       2021-03-17              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 深度学习
---
# 一、神经网络基础
## 1. 简介
- 人工神经网络
	- 也简称为神经网络（NN），是一种模仿生物神经网络结构和功能的 计算模型。
- 常用的神经网络
	- 卷积神经网络(Convolutional Neural Network)
	- 循环神经网络(Recurrent Neural Network)
	- 生成对抗网络(Generative Adversarial Networks)
	- 深度强化学习(Deep Reinforcement Learning)

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0j6rtwqj30im0aj458.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0iuf47kj30f6076q6h.jpg)

## 2. 结构分层
- 结构分层：
	- 输入层：即输入x的那一层
	- 输出层：即输出y的那一层
	- 隐藏层：输入层和输出层之间都是隐藏层
- 特点
	- 同一层的神经元之间没有连接。
	- 第N层的每个神经元和第N-1层的所有神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。
	- 每个连接都有一个权值。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0k6bzj0j30jm09yjue.jpg)

# 二. 神经网络如何工作
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0lr12gvj30ld0jwq56.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0ml19haj30ic07yt9s.jpg)
## 1. 激活函数定义
- 本质是向神经网络中引入**非线性因素**的，通过激活函数，神经网络就可以拟合各种曲线，可以逼近任意函数。
- 如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合；

## 2. 常见激活函数

### 2.1 Sigmoid/logistics函数
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0pb9k5kj30lk0iracs.jpg)

```python
# 导入相应的工具包

import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import numpy as np
# 定义x的取值范围

x = np.linspace(-10, 10, 100)
# 直接使用tensorflow实现

y = tf.nn.sigmoid(x)
# 绘图

plt.plot(x,y)
plt.grid()
```

### 2.2 tanh(双曲正切曲线)
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0qcy6gxj30ld0kjq5g.jpg)

```python
# 导入相应的工具包

import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import numpy as np
# 定义x的取值范围

x = np.linspace(-10, 10, 100)
# 直接使用tensorflow实现

y = tf.nn.tanh(x)
# 绘图

plt.plot(x,y)
plt.grid()
```

### 2.3 RELU
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0ral9kbj30lc0n4wi9.jpg)

```python
# 导入相应的工具包
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import numpy as np
# 定义x的取值范围
x = np.linspace(-10, 10, 100)
# 直接使用tensorflow实现
y = tf.nn.relu(x)
# 绘图
plt.plot(x,y)
plt.grid()

```

### 2.4 LeakReLu
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0s2a79tj30hr0d5759.jpg)

```python
# 导入相应的工具包

import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import numpy as np
# 定义x的取值范围

x = np.linspace(-10, 10, 100)
# 直接使用tensorflow实现

y = tf.nn.leaky_relu(x)
# 绘图

plt.plot(x,y)
plt.grid()
```

### 2.5 SoftMax
> 可以看作是归一化操作，但不是等比例的

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0tjo0hvj30lr0hsjvg.jpg)

```python
# 导入相应的工具包

import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import numpy as np
# 数字中的score

x = tf.constant([0.2,0.02,0.15,1.3,0.5,0.06,1.1,0.05,3.75])
# 将其送入到softmax中计算分类结果

y = tf.nn.softmax(x) 
# 将结果进行打印

print(y)
```

### 2.6 其他激活函数
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0uwya93j30l40i3gri.jpg)

### 2.7 如何选择激活函数

- 隐藏层
	- 优先选择RELU激活函数
	- 如果ReLu效果不好，那么尝试其他激活，如Leaky ReLu等。
	- 如果你使用了Relu， 需要注意一下Dead Relu问题， 避免出现大的梯度从而导致过多的神经元死亡。
	- 不要使用sigmoid激活函数，可以尝试使用tanh激活函数
- 输出层
	- 二分类问题选择sigmoid激活函数
	- 多分类问题选择softmax激活函数
	- 回归问题选择identity激活函数

## 3. 参数初始化
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gon0x68w1kj30l00fyjta.jpg)

### 3.1 随机初始化
随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化。

### 3.2 标准初始化
权重参数初始化从区间均匀随机取值。即在(-1/√d,1/√d)均匀分布中生成当前神经元的权重，其中d为每个神经元的输入数量。

### 3.3.Xavier初始化
