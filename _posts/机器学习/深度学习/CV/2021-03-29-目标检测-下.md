---
layout:     post                    # 使用的布局（不需要改）
title:      图像检测-下		        # 标题 
subtitle:   YOLO,YOLO-V2,YOLO-V3,YOLO-V4   	# 副标题
date:       2021-03-29              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - CV
---
# 一、YOLO算法
Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别，整个系统如下图所示：
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp111ep5ipj30lu07pn29.jpg)
## 1. 算法思想
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp112hc2wdj30lt0b0dhz.jpg)
Yolo意思是You Only Look Once，它并没有真正的去掉候选区域，而是创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。

Yolo模型采用预定义预测区域的方法来完成目标检测，具体而言是将原始图像划分为 7x7=49 个网格（grid），每个网格允许预测出2个边框（bounding box，包含某个对象的矩形框），总共 49x2=98 个bounding box。我们将其理解为98个预测区，很粗略的覆盖了图片的整个区域，就在这98个预测区中进行目标检测。



## 2. 网络架构
YOLO的结构非常简单，就是单纯的卷积、池化最后加了两层全连接，从网络结构上看，与前面介绍的CNN分类网络没有本质的区别，最大的差异是输出层用线性函数做激活函数，因为需要预测bounding box的位置（数值型），而不仅仅是对象的概率。所以粗略来说，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量，如下图所示：

![](https://tva1.sinaimg.cn/large/008eGmZEly1gp1135bpjhj30ly0cg78i.jpg)

### 2.1 网格输入
网络的输入是原始图像，唯一的要求是缩放到448x448的大小。主要是因为Yolo的网络中，卷积层最后接了两个全连接层，全连接层是要求固定大小的向量作为输入，所以Yolo的输入图像的大小固定为448x448。

### 2.2 网格输出
网络的输出就是一个7x7x30 的张量（tensor）
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp1147m873j30m50g2jx1.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp114lix34j30lt0gqdns.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp114tk5myj30lb0brgnx.jpg)

## 3. 模型训练
在进行模型训练时，我们需要构造训练样本和设计损失函数，才能利用梯度下降对网络进行训练。
### 3.1 训练样本的构建
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp115tug0hj30l50fgala.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp1165o7unj30ll0hoadu.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp116f41h4j30l106gdh7.jpg)

### 3.2 损失函数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp116vqmslj30li09bq57.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp1177zy9nj30lu0fr435.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp117giz1xj30kz0cnafe.jpg)
### 3.3 模型训练
Yolo先使用ImageNet数据集对前20层卷积网络进行预训练，然后使用完整的网络，在PASCAL VOC数据集上进行对象识别和定位的训练。
Yolo的最后一层采用线性激活函数，其它层都是Leaky ReLU。训练中采用了drop out和数据增强（data augmentation）来防止过拟合.

## 4. 模型预测
将图片resize成448x448的大小，送入到yolo网络中，输出一个 7x7x30 的张量（tensor）来表示图片中所有网格包含的对象（概率）以及该对象可能的2个位置（bounding box）和可信程度（置信度）。在采用NMS（Non-maximal suppression，非极大值抑制）算法选出最有可能是目标的结果。

## 5. YOLO总结
- 优点：
	- 速度非常快，处理速度可以达到45fps，其快速版本（网络较小）甚至可以达到155fps。
	- 训练和预测可以端到端的进行，非常简便。
- 缺点：
	- 准确率会打折扣
	- 对于小目标和靠的很近的目标检测效果并不好

# 二、YOLO-V2
## 1. better(从预测更准确)
- batch normalization
	- 批标准化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果，从而能够获得更好的收敛速度和收敛效果。
- 使用高分辨率图像微调分类模型
	- 采用 224x224 图像进行分类模型预训练后，再采用 448x448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448x448 的分辨率。然后再使用 448x448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。
- 采用Anchor Boxes
	- YOLO2每个grid采用5个先验框
- 聚类提取anchor尺度
	- YOLO2尝试统计出更符合样本中对象尺寸的先验框，这样就可以减少网络微调先验框到实际位置的难度;
	- YoloV2选择了聚类的五种尺寸最常使用的anchor box。
- 边框位置的预测
	- ![](https://tva1.sinaimg.cn/large/008eGmZEly1gp11hgyskhj30gi07l3z7.jpg)

- 细粒度特征融合
	- ![](https://tva1.sinaimg.cn/large/008eGmZEly1gp11ht68ujj30ll0cptbp.jpg)

- 多尺度训练
	- YOLO2中没有全连接层，可以输入任何尺寸的图像。

## 2. faster(速度更快)
yoloV2提出了Darknet-19（有19个卷积层和5个MaxPooling层）网络结构作为特征提取网络。DarkNet-19比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约⅕，以保证更快的运算速度。
![](https://tva1.sinaimg.cn/large/008eGmZEly1gp11j2e9cbj30la0cn0w2.jpg)

## 3. stronger(识别对象更多)
VOC数据集可以检测20种对象，但实际上对象的种类非常多，只是缺少相应的用于对象检测的训练样本。YOLO2尝试利用ImageNet非常大量的分类样本，联合COCO的对象检测数据集一起训练，使得YOLO2即使没有学过很多对象的检测样本，也能检测出这些对象。

# 三、YOLO-V3
yoloV3以V1，V2为基础进行的改进，主要有：利用多尺度特征进行目标检测；先验框更丰富；调整了网络结构；对象分类使用logistic代替了softmax,更适用于多标签分类任务。
## 1. 算法简介
YOLOv3是YOLO (You Only Look Once)系列目标检测算法中的第三版，相比之前的算法，尤其是针对小目标，精度有显著提升。
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2xz4spgoj30lr0ck7bp.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2xzeulurj30lj0aigpt.jpg)

## 2. 多尺度检测
通常一幅图像包含各种不同的物体，并且有大有小。比较理想的是一次就可以将所有大小的物体同时检测出来。因此，网络必须具备能够“看到”不同大小的物体的能力。因为网络越深，特征图就会越小，所以网络越深小的物体也就越难检测出来。

在实际的feature map中，随着网络深度的加深，浅层的feature map中主要包含低级的信息（物体边缘，颜色，初级位置信息等），深层的feature map中包含高等信息（例如物体的语义信息：狗，猫，汽车等等）。因此在不同级别的feature map对应不同的scale，所以我们可以在不同级别的特征图中进行目标检测。如下图展示了多种scale变换的经典方法。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2xzy6irvj30lm0ctaj5.jpg)

(a) 这种方法首先建立图像金字塔，不同尺度的金字塔图像被输入到对应的网络当中，用于不同scale物体的检测。但这样做的结果就是每个级别的金字塔都需要进行一次处理，速度很慢。

(b) 检测只在最后一层feature map阶段进行，这个结构无法检测不同大小的物体

(c) 对不同深度的feature map分别进行目标检测。SSD中采用的便是这样的结构。这样小的物体会在浅层的feature map中被检测出来，而大的物体会在深层的feature map被检测出来，从而达到对应不同scale的物体的目的，缺点是每一个feature map获得的信息仅来源于之前的层，之后的层的特征信息无法获取并加以利用。

(d) 与©很接近，但不同的是，当前层的feature map会对未来层的feature map进行上采样，并加以利用。因为有了这样一个结构，当前的feature map就可以获得“未来”层的信息，这样的话低阶特征与高阶特征就有机融合起来了，提升检测精度。在YOLOv3中，就是采用这种方式来实现目标多尺度的变换的。

## 3. 网络模型结构
在基本的图像特征提取方面，YOLO3采用了Darknet-53的网络结构（含有53个卷积层），它借鉴了残差网络ResNet的做法，在层之间设置了shortcut，来解决深层网络梯度的问题，shortcut如下图所示：包含两个卷积层和一个shortcut connections。
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y11rox4j30lf0bkn0x.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y1b0vngj30le0eh49h.jpg)
下面我们看下网络结构：

基本组件：蓝色方框内部分
1、CBL：Yolov3网络结构中的最小组件，由Conv+Bn+Leaky_relu激活函数三者组成。 2、Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。 3、ResX：由一个CBL和X个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是416->208->104->52->26->13大小。

其他基础操作：
1、Concat：张量拼接，会扩充两个张量的维度，例如26×26×256和26×26×512两个张量拼接，结果是26×26×768。
2、Add：张量相加，张量直接相加，不会扩充维度，例如104×104×128和104×104×128相加，结果还是104×104×128。

Backbone中卷积层的数量：
每个ResX中包含1+2×X个卷积层，因此整个主干网络Backbone中一共包含1+（1+2×1）+（1+2×2）+（1+2×8）+（1+2×8）+（1+2×4）=52，再加上一个FC全连接层，即可以组成一个Darknet53分类网络。不过在目标检测Yolov3中，去掉FC层，仍然把Yolov3的主干网络叫做Darknet53结构。

## 4. 先验框
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y2lbwigj30ll0kbdtr.jpg)

## 5. logistic回归
预测对象类别时不使用softmax，而是被替换为一个1x1的卷积层+logistic激活函数的结构。使用softmax层的时候其实已经假设每个输出仅对应某一个单个的class，但是在某些class存在重叠情况（例如woman和person）的数据集中，使用softmax就不能使网络对数据进行很好的预测。
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y3lsbakj30mh0gqwjt.jpg)

## 6. 模型的输入与输出
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y3w6zwwj30lw0js442.jpg)


# 四、YOLO-V4
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2y4j8b65j30mc0fw46t.jpg)

- Yolov4的结构图和Yolov3是相似的，不过使用各种新的算法思想对各个子结构都进行了改进。 先整理下Yolov4的结构组件
	- 基本组件：
		- CBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。
		- CBL：由Conv+Bn+Leaky_relu激活函数三者组成。
		- Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。
		- CSPX：由三个卷积层和X个Res unint模块Concate组成。
		- SPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。
	- 其他基础操作：
		- Concat：张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。
		- Add：张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。
		- Backbone中卷积层的数量： 每个CSPX中包含3+2×X个卷积层，因此整个主干网络Backbone中一共包含2+（3+2×1）+2+（3+2×2）+2+（3+2×8）+2+（3+2×8）+2+（3+2×4）+1=72。

注意：
网络的输入大小不是固定的，在yoloV3中输入默认是416×416，在yoloV4中默认是608×608，在实际项目中也可以根据需要修改，比如320×320，一般是32的倍数。 输入图像的大小和最后的三个特征图的大小也是对应的，比如416×416的输入，最后的三个特征图大小是13×13，26×26，52×52， 如果是608×608，最后的三个特征图大小则是19×19，38×38，76×76。

# 五、 YOLO-V3案例
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2yb5zrb6j30hc0n5ahg.jpg)

## 1. 数据获取
根据要实现的业务场景，需要收集大量的图像数据，一般来说包含两大来源，一部分是网络数据，可以是开源数据，也可以通过百度、Google图片爬虫得到，另一部分是用户场景的视频录像，这一部分的数据量会更大。对于开源数据我们不需要进行标注，而爬取的数据和视频录像需要进行标注，这时我们可以使用开源工具labelImg进行标注，该软件截图如下：
![](https://tva1.sinaimg.cn/large/008eGmZEgy1gp2ybogvzuj30lg0fy48a.jpg)

mac下配置labelImg的方法：
```sh
# macos安装方法
# 1.创建虚拟环境，指定python版本，必须是python3
conda create -n labelImage python=3.6
# 2. 开启虚拟环境
conda activate labelImage
# 3.安装pyqt5
pip install pyqt5 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com
# 4.安装lxml
pip install lxml -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com
# 5.安装labelImg
pip install labelImg -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com
 
# 使用方法：
# 1.终端中启动，开启虚拟环境
conda activate labelImage
# 2.启动labelImg
labelImg
# 3.接下来就可以进行图像标注了

```

## 2. TFRecord文件
该案例中我们依然使用VOC数据集来进行目标检测，不同的是我们要利用tfrecord文件来存储和读取数据，首先来看一下tfrecord文件的相关内容。

为什么要使用tfrecord文件？

TFRecord是Google官方推荐使用的数据格式化存储工具，为TensorFlow量身打造的。
TFRecord规范了数据的读写方式，数据读取和处理的效率都会得到显著的提高

## 3. 模型构建

## 4. 模型预测

## 5. 模型预测