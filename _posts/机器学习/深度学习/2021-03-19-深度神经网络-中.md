---
layout:     post                    # 使用的布局（不需要改）
title:      深度神经网络-中		    # 标题 
subtitle:   梯度下降、反向传播、动量算法    # 副标题
date:       2021-03-17              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 深度学习
---

# 一、深度学习的优化方法

## 1. 梯度下降算法
> 梯度下降法简单来说就是一种寻找使损失函数最小化的方法;

![](https://tva1.sinaimg.cn/large/008eGmZEly1gopf2kum76j30o40f9789.jpg)

### 1.1 梯度下降法的分类
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopf346a8cj30o006tn29.jpg)

### 1.2 方法实现

- tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', \*\*kwargs)

```python
# 导入相应的工具包

import tensorflow as tf
# 实例化优化方法：SGD 

opt = tf.keras.optimizers.SGD(learning_rate=0.1)
# 定义要调整的参数

var = tf.Variable(1.0)
# 定义损失函数：无参但有返回值

loss = lambda: (var ** 2)/2.0  
# 计算梯度，并对参数进行更新，步长为 `- learning_rate * grad`

opt.minimize(loss, [var]).numpy()
# 展示参数更新结果

var.numpy()
```

### 1.3 三个基础概念
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopfakjsv4j30o80ip79a.jpg)

## 2. 反向传播算法（BP算法）
### 2.1 前向传播
- 前向传播指的是数据输入的神经网络中，逐层向前传输，一直到运算到输出层为止；

![](https://tva1.sinaimg.cn/large/008eGmZEly1gopfiycu9uj30mp0fbq4z.jpg)

### 2.2 链式法则
- 反向传播算法是利用链式法则进行梯度求解及权重更新的。
- 对于复杂的复合函数，我们将其拆分为一系列的加减乘除或指数，对数，三角函数等初等函数，通过链式法则完成复合函数的求导。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gopfnywfcoj30o20k1dif.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopfowvbghj30nz0n50z5.jpg)

### 2.3 反向传播算法
- 反向传播算法利用链式法则对神经网络中的各个节点的权重进行更新
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg0cvednj30mt0ffq67.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg0t3rzrj30ko0do0ve.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg16j04mj30je0kwq50.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg1i2zhvj30nt0j6djw.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg1rqe52j30n20l4afe.jpg)


## 3. 梯度下降算法的优化方法
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopg2vke0xj30o10ezwj4.jpg)

### 3.1 动量算法
> 动量算法主要解决鞍点问题。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gopgiqq4hjj30o60amjsl.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gopgiyzks0j30nw0b5n0r.jpg)

```python
# 导入相应的工具包

import tensorflow as tf
# 实例化优化方法：SGD 指定参数beta=0.9

opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
# 定义要调整的参数，初始值

var = tf.Variable(1.0)
val0 = var.value()
# 定义损失函数

loss = lambda: (var ** 2)/2.0         
#第一次更新：计算梯度，并对参数进行更新，步长为 `- learning_rate * grad`

opt.minimize(loss, [var]).numpy()
val1 = var.value()
# 第二次更新：计算梯度，并对参数进行更新，因为加入了momentum,步长会增加

opt.minimize(loss, [var]).numpy()
val2 = var.value()
# 打印两次更新的步长

print("第一次更新步长={}".format((val0 - val1).numpy()))
print("第二次更新步长={}".format((val1 - val2).numpy()))
```

### 3.2 AdaGrad
> 自适应学习率算法

AdaGrad算法会使用一个小批量随机梯度g_t,g_t按元素平方的累加变量st。在首次迭代时，AdaGrad将s0中每个元素初始化为0。在t次迭代，首先将小批量随机梯度gt按元素平方后累加到变量st：

![](https://tva1.sinaimg.cn/large/008eGmZEly1gopglbi31fj30kw077t9t.jpg)

其中α是学习率，ϵ是为了维持数值稳定性而添加的常数，如10^-6,。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率；

```python
# 导入相应的工具包

import tensorflow as tf
# 实例化优化方法：SGD

opt = tf.keras.optimizers.Adagrad(
    learning_rate=0.1, initial_accumulator_value=0.1, epsilon=1e-07
)
# 定义要调整的参数

var = tf.Variable(1.0)
# 定义损失函数：无参但有返回值

def loss(): return (var ** 2)/2.0

# 计算梯度，并对参数进行更新，

opt.minimize(loss, [var]).numpy()
# 展示参数更新结果

var.numpy()
```

### 3.3 RMSprop

### 3.4 Adam


