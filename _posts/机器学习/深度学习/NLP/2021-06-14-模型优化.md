---
layout:     post                    # 使用的布局（不需要改）
title:      模型优化		# 标题 		  
subtitle:   模型剪枝、模型量化、知识蒸馏		#副标题
date:       2021-04-25              # 时间
author:     新宇                    # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - NLP
---
# 一、 模型剪枝
## 1. 为什么要做模型剪枝
- 大模型容易train, 小模型难train（欠拟合）
- 受限于移动端的存储和算力，服务端随意

## 2. 架构图
![](https://tva1.sinaimg.cn/large/008i3skNly1grlnsi5qd2j30ln0ftag4.jpg)


# 二、知识蒸馏

## 1. 什么是知识蒸馏

- train一个小模型模仿大模型的输出
- 因为大模型的输出更丰富，用小模型输出的dist逼近大模型的dist


## 2. 架构图

![](https://tva1.sinaimg.cn/large/008i3skNly1grlovd31ykj30m60gbaf4.jpg)

- 集成模型

![](https://tva1.sinaimg.cn/large/008i3skNly1grlp1f49vej30m30g8gqk.jpg)

- 为了让student能轻易的学到teacher，需要将teacher的output拉近一点（经实验证明并没什么用）

![](https://tva1.sinaimg.cn/large/008i3skNly1grlp51tqnfj30m30g9779.jpg)


# 三、参数量化

## 1. 参数量化流程

- 参数聚类
- 各类参数求平均
- 用更少的参数表示原来的weights
- 或者通过哈夫曼编码表示

## 2. 架构图
![](https://tva1.sinaimg.cn/large/008i3skNly1grlpagzm7vj30lq0f4n32.jpg)

# 四、调整模型架构

## 1. FC
![](https://tva1.sinaimg.cn/large/008i3skNly1grlpgtdav5j30n90g2whj.jpg)

## 2. CNN
![](https://tva1.sinaimg.cn/large/008i3skNly1grlpm9ugw3j30lm0gf77q.jpg)
![](https://tva1.sinaimg.cn/large/008i3skNly1grlplupizyj30lx0gan1n.jpg)

- 又如 一个5*5的卷积核可以由两个3*3的卷积核替代，参数量减少了 5*5 - 2*3*3 = 7