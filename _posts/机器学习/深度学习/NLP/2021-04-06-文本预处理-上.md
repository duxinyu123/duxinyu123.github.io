---
layout:     post                    # 使用的布局（不需要改）
title:      文本预处理-上		        # 标题 
subtitle:     	# 副标题
date:       2021-04-06              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - NLP
---
# 一、NLP入门
## 1. 定义
自然语言处理（Natural Language Processing, 简称NLP）是计算机科学与语言学中关注于计算机与人类语言间转换的领域.

## 2. 发展简史
此处省略

## 3. 应用场景
- 语音助手
- 机器翻译
- 搜索引擎
- 智能问答

# 二、文本处理的基本方法

## 1. 分词
### 1.1 什么是分词

- 分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符, 分词过程就是找到这样分界符的过程.
- 分词的作用:
	- 词作为语言语义理解的最小单元, 是人类理解文本语言的基础. 因此也是AI解决NLP领域高阶任务, 如自动问答, 机器翻译, 文本生成的重要基础环节.
- 流行中文分词工具jieba:
	- 愿景: “结巴”中文分词, 做最好的 Python 中文分词组件.
	- 安装jieba: pip install jieba
	- jieba的特性:
		- 支持多种分词模式
			- 精确模式
			- 全模式
			- 搜索引擎模式
		- 支持中文繁体分词
		- 支持用户自定义词典


举个栗子:

```text
工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作 

==> 

['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
```
### 1.2 jieba的使用

#### 1.2.1 精确模式分词

```python
# 试图将句子最精确地切开，适合文本分析.

>>> import jieba
>>> content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
>>> jieba.cut(content, cut_all=False)  # cut_all默认为False

# 将返回一个生成器对象

<generator object Tokenizer.cut at 0x7f065c19e318>

# 若需直接返回列表内容, 使用jieba.lcut即可

>>> jieba.lcut(content, cut_all=False)
['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']

```

#### 1.2.2 全模式分词
把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能消除 歧义
```python
>>> import jieba
>>> content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
>>> jieba.cut(content, cut_all=True)  # cut_all默认为False

# 将返回一个生成器对象

<generator object Tokenizer.cut at 0x7f065c19e318>

# 若需直接返回列表内容, 使用jieba.lcut即可

>>> jieba.lcut(content, cut_all=True)
['工信处', '处女', '女干事', '干事', '每月', '月经', '经过', '下属', '科室', '都', '要', '亲口', '口交', '交代', '24', '口交', '交换', '交换机', '换机', '等', '技术', '技术性', '性器', '器件', '的', '安装', '安装工', '装工', '工作']
```

#### 1.2.3 搜索引擎模式分词
在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词
```python
>>> import jieba
>>> content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
>>> jieba.cut_for_search(content)

# 将返回一个生成器对象

<generator object Tokenizer.cut at 0x7f065c19e318>

# 若需直接返回列表内容, 使用jieba.lcut_for_search即可

>>> jieba.lcut_for_search(content)
['工信处', '干事', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换', '换机', '交换机', '等', '技术', '技术性', '器件', '的', '安装', '工作']

# 对'女干事', '交换机'等较长词汇都进行了再次分词.
```


#### 1.2.4 中文繁体分词

```python
>>> import jieba
>>> content = "煩惱即是菩提，我暫且不提"
>>> jieba.lcut(content)
['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
```

#### 1.2.5 使用用户自定义词典
- 添加自定义词典后, jieba能够准确识别词典中出现的词汇，提升整体的识别准确率
	- 词典格式: 每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒.
	- 词典样式如下, 具体词性含义请参照附录: jieba词性对照表, 将该词典存为userdict.txt, 方便之后加载使用.

```text
云计算 5 n
李小福 2 nr
easy_install 3 eng
好用 300
韩玉赏鉴 3 nz
八一双鹿 3 nz
```

```python
>>> import jieba
>>> jieba.lcut("八一双鹿更名为八一南昌篮球队！")
# 没有使用用户自定义词典前的结果:

>>> ['八', '一双', '鹿', '更名', '为', '八一', '南昌', '篮球队', '！']


>>> jieba.load_userdict("./userdict.txt")
# 使用了用户自定义词典后的结果:

['八一双鹿', '更名', '为', '八一', '南昌', '篮球队', '！']
```
### 1.3 流行中英文分词工具hanlp
- 中英文NLP处理工具包, 基于tensorflow2.0, 使用在学术界和行业中推广最先进的深度学习技术.
- 使用pip进行安装
	- pip install hanlp

#### 1.3.1 使用hanlp进行中文分词
```python
>>> import hanlp
# 加载CTB_CONVSEG预训练模型进行分词任务
>>> tokenizer = hanlp.load('CTB6_CONVSEG')
>>> tokenizer("工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作")
['工信处', '女', '干事', '每', '月', '经过', '下', '属', '科室', '都', '要', '亲口', '交代', '24口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']

```

#### 1.3.2 使用hanlp进行英文分词
```python
# 进行英文分词, 英文分词只需要使用规则即可

>>> tokenizer = hanlp.utils.rules.tokenize_english 
>>> tokenizer('Mr. Hankcs bought hankcs.com for 1.5 thousand dollars.')
['Mr.', 'Hankcs', 'bought', 'hankcs.com', 'for', '1.5', 'thousand', 'dollars', '.']
```

## 2. 命名实体识别

