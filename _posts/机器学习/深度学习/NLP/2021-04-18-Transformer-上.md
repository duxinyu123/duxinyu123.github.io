---
layout:     post                    # 使用的布局（不需要改）
title:      Transformer-上		        # 标题 
subtitle:   整体介绍、架构图  	# 副标题
date:       2021-04-17              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - NLP
---
# 一、Transform背景介绍
- 2018年10月，Google发出一篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》, BERT模型横空出世, 并横扫NLP领域11项任务的最佳成绩!
- 论文地址: https://arxiv.org/pdf/1810.04805.pdf
- 而在BERT中发挥重要作用的结构就是Transformer, 之后又相继出现XLNET，roBERT等模型击败了BERT，但是他们的核心没有变，仍然是：Transformer.

![](https://tva1.sinaimg.cn/large/008eGmZEly1gpo38636g0j30lg0don0p.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gpo38pe9pkj30kz0ekaer.jpg)


# 二、整体架构图
