---
layout:     post                    # 使用的布局（不需要改）
title:      Pytorch		        # 标题 
subtitle:   Pytorch基本介绍  	# 副标题
date:       2021-04-01              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - NLP
---
# 一、Pytorch基本介绍
## 1. 什么是Pytorch
- Pytorch是一个基于Numpy的科学计算包, 向它的使用者提供了两大功能.
- 作为Numpy的替代者, 向用户提供使用GPU强大功能的能力.
- 做为一款深度学习的平台, 向用户提供最大的灵活性和速度.

## 2. Pytorch的基本元素操作

```python
from __future__ import print_function
import torch

# 创建一个没有初始化的矩阵:

x = torch.empty(5, 3)
print(x)

# 创建一个有初始化的矩阵:

x = torch.rand(5, 3)
print(x)

#  创建一个全零矩阵并可指定数据元素的类型为long

x = torch.zeros(5, 3, dtype=torch.long)
print(x)

# 直接通过数据创建张量

x = torch.tensor([2.5, 3.5])
print(x)

# 通过已有的一个张量创建相同尺寸的新张量
# 利用news_methods方法得到一个张量

x = x.new_ones(5, 3, dtype=torch.double)
print(x)

# 利用randn_like方法得到相同张量尺寸的一个新张量, 并且采用随机初始化来对其赋值

y = torch.randn_like(x, dtype=torch.float)
print(y)

# 得到张量的尺寸

print(x.size())
```

## 3. Pytorch基本运算操作

```python
# 加法操作

y = torch.rand(5, 3)
print(x + y)

# 第二种方式

print(torch.add(x, y))

# 第三种方式
# 提前设定一个空的张量

result = torch.empty(5, 3)
# 将空的张量作为加法的结果存储张量

torch.add(x, y, out=result)
print(result)

# 第四种方式：原地置换

y.add_(x)
print(y)

# 用类似于Numpy的方式对张量进行操作

print(x[:, 1])

# 改变张量的形状: torch.view()

x = torch.randn(4, 4)
# tensor.view()操作需要保证数据元素的总数量不变

y = x.view(16)
# -1代表自动匹配个数

z = x.view(-1, 8)
print(x.size(), y.size(), z.size())

# 如果张量中只有一个元素, 可以用.item()将值取出, 作为一个python number

x = torch.randn(1)
print(x)
print(x.item())
```

# 4. Torch Tensor和Numpy array之间的相互转换

```python
a = torch.ones(5)
print(a)
b = a.numpy()
print(b)

a.add_(1)
print(a)
print(b)

# 将Numpy array转换为Torch Tensor

import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)
```
使用GPU运行

```python
# 如果服务器上已经安装了GPU和CUDA

if torch.cuda.is_available():
    # 定义一个设备对象, 这里指定成CUDA, 即使用GPU

    device = torch.device("cuda")
    # 直接在GPU上创建一个Tensor

    y = torch.ones_like(x, device=device)
    # 将在CPU上面的x张量移动到GPU上面

    x = x.to(device)
    # x和y都在GPU上面, 才能支持加法运算

    z = x + y
    # 此处的张量z在GPU上面

    print(z)
    # 也可以将z转移到CPU上面, 并同时指定张量元素的数据类型

    print(z.to("cpu", torch.double))

```
# 二、Pytorch中的autograd

## 1. 关于torch.Tensor
- torch.Tensor是整个package中的核心类, 如果将属性.requires_grad设置为True, 它将追踪在这个类上定义的所有操作. 当代码要进行反向传播的时候, 直接调用.backward()就可以自动计算所有的梯度. 在这个Tensor上的所有梯度将被累加进属性.grad中.
- 如果想终止一个Tensor在计算图中的追踪回溯, 只需要执行.detach()就可以将该Tensor从计算图中撤下, 在未来的回溯计算中也不会再计算该Tensor.
- 除了.detach(), 如果想终止对计算图的回溯, 也就是不再进行方向传播求导数的过程, 也可以采用代码块的方式with torch.no_grad():, 这种方式非常适用于对模型进行预测的时候, 因为预测阶段不再需要对梯度进行计算.

## 2. 关于torch.Function
- Function类是和Tensor类同等重要的一个核心类, 它和Tensor共同构建了一个完整的类, 每一个Tensor拥有一个.grad_fn属性, 代表引用了哪个具体的Function创建了该Tensor.
- 如果某个张量Tensor是用户自定义的, 则其对应的grad_fn is None.

## 3. 代码

```python
x1 = torch.ones(3, 3)
print(x1)

x = torch.ones(2, 2, requires_grad=True)
print(x)

y = x + 2
print(y)

print(x.grad_fn)
print(y.grad_fn)

z = y * y * 3
out = z.mean()
print(z, out)

a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
```

## 4. 关于梯度Gradient
```python
out.backward()
print(x.grad)

print(x.requires_grad)
print((x ** 2).requires_grad)

# 关于自动求导的属性设置: 可以通过设置.requires_grad=True来执行自动求导, 也可以通过代码块的限制来停止自动求导.

with torch.no_grad():
    print((x ** 2).requires_grad)

# 可以通过.detach()获得一个新的Tensor, 拥有相同的内容但不需要自动求导.

print(x.requires_grad)
y = x.detach()
print(y.requires_grad)
print(x.eq(y).all())
```

# 三、Pytorch初步应用
## 1. 构建神经网络的流程

- 构建神经网络的典型流程:
- 定义一个拥有可学习参数的神经网络
- 遍历训练数据集
- 处理输入数据使其流经神经网络
- 计算损失值
- 将网络参数的梯度进行反向传播
- 以一定的规则更新网络的权重

## 2. Pytorch实现的神经网络
```python
# 导入若干工具包

import torch
import torch.nn as nn
import torch.nn.functional as F


# 定义一个简单的网络类

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 定义第一层卷积神经网络, 输入通道维度=1, 输出通道维度=6, 卷积核大小3*3

        self.conv1 = nn.Conv2d(1, 6, 3)
        # 定义第二层卷积神经网络, 输入通道维度=6, 输出通道维度=16, 卷积核大小3*3

        self.conv2 = nn.Conv2d(6, 16, 3)
        # 定义三层全连接网络

        self.fc1 = nn.Linear(16 * 6 * 6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 在(2, 2)的池化窗口下执行最大池化操作

        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        # 计算size, 除了第0个维度上的batch_size

        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)

```

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())

input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)

# 有了输出张量后, 就可以执行梯度归零和反向传播的操作了.

net.zero_grad()
out.backward(torch.randn(1, 10))
```

## 3. 损失函数
- 损失函数的输入是一个输入的pair: (output, target), 然后计算出一个数值来评估output和target之间的差距大小.
- 在torch.nn中有若干不同的损失函数可供使用, 比如nn.MSELoss就是通过计算均方差损失来评估输入和目标值之间的差距.

```python
output = net(input)
target = torch.randn(10)

# 改变target的形状为二维张量, 为了和output匹配

target = target.view(1, -1)
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
```

关于方向传播的链条: 如果我们跟踪loss反向传播的方向, 使用.grad_fn属性打印, 将可以看到一张完整的计算图如下:

```python
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
```

当调用loss.backward()时, 整张计算图将对loss进行自动求导, 所有属性requires_grad=True的Tensors都将参与梯度求导的运算, 并将梯度累加到Tensors中的.grad属性中.

```python
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU

```

## 4. 反向传播
```python
# Pytorch中执行梯度清零的代码

net.zero_grad()

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

# Pytorch中执行反向传播的代码

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

## 5. 更新网络参数
更新参数最简单的算法就是SGD(随机梯度下降).
具体的算法公式表达式为: weight = weight - learning_rate * gradient

```python
# 首先用传统的Python代码来实现SGD如下

learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)

# 然后使用Pytorch官方推荐的标准代码如下
# 首先导入优化器的包, optim中包含若干常用的优化算法, 比如SGD, Adam等
import torch.optim as optim

# 通过optim创建优化器对象

optimizer = optim.SGD(net.parameters(), lr=0.01)

# 将优化器执行梯度清零的操作

optimizer.zero_grad()

output = net(input)
loss = criterion(output, target)

# 对损失值执行反向传播的操作

loss.backward()
# 参数的更新通过一行标准代码来执行

optimizer.step()
```

# 四、使用Pytorch构建一个分类器

## 1. 下载CIFAR10数据集
```python
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

## 2. 展示训练集的图片
```python
# 导入画图包和numpy

import matplotlib.pyplot as plt
import numpy as np

# 构建展示图片的函数

def imshow(img):
    img = img / 2 + 0.5
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# 从数据迭代器中读取一张图片

dataiter = iter(trainloader)
images, labels = dataiter.next()

# 展示图片

imshow(torchvision.utils.make_grid(images))
# 打印标签label

print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

## 3. 定义卷积神经网络

```python
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
```

## 4. 定义损失函数
```python
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

```

## 5. 在训练集上训练模型

```python
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # data中包含输入图像张量inputs, 标签张量labels

        inputs, labels = data

        # 首先将优化器梯度归零

        optimizer.zero_grad()

        # 输入图像张量进网络, 得到输出张量outputs

        outputs = net(inputs)

        # 利用网络的输出outputs和标签labels计算损失值

        loss = criterion(outputs, labels)

        # 反向传播+参数更新, 是标准代码的标准流程

        loss.backward()
        optimizer.step()

        # 打印轮次和损失值

        running_loss += loss.item()
        if (i + 1) % 2000 == 0:
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

# 保存模型
# 首先设定模型的保存路径

PATH = './cifar_net.pth'
# 保存模型的状态字典

torch.save(net.state_dict(), PATH)
```

## 6. 在测试集上测试模型

```python
dataiter = iter(testloader)
images, labels = dataiter.next()

# 打印原始图片

imshow(torchvision.utils.make_grid(images))
# 打印真实的标签

print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))

# 加载模型并对测试图片进行预测

# 首先实例化模型的类对象

net = Net()
# 加载训练阶段保存好的模型的状态字典

net.load_state_dict(torch.load(PATH))

# 利用模型对图片进行预测

outputs = net(images)

# 共有10个类别, 采用模型计算出的概率最大的作为预测的类别

_, predicted = torch.max(outputs, 1)

# 打印预测标签的结果

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))
```

## 7. 模型评估
```python
# 在全部测试集上的表现

correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))

# 在每个类别上的表现

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
```

## 8. 在GPU上训练模型
```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(device)

# 将模型转移到GPU上

net.to(device)

# 将输入的图片张量和标签张量转移到GPU上

inputs, labels = data[0].to(device), data[1].to(device)
```