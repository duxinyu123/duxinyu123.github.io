---
layout:     post                    # 使用的布局（不需要改）
title:      集成学习进阶  			    # 标题 
subtitle:   XGBoost、lightGBM	    # 副标题
date:       2020-03-05              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---
# 一、XGBoost
## 1. 介绍
- XGBoost(Extreme Gradient Boosting)全名叫极端梯度提升树，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者 用了XGBoost。
- XGBoost在绝大多数的回归和分类问题上表现的十分顶尖

## 2. 最优模型构建方法
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9h774tyj30mj0g0ae3.jpg)

## 3. XGBoost的目标函数推导
### 3.1 目标函数的确定
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9ja159ij30lg05x3zz.jpg)

### 3.2 CART树的介绍
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9jp8ps0j30h70bnmzp.jpg)

## 4. 树的复杂度定义
### 4.1 定义每课树的复杂度
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9miiyikj30hy04ndgs.jpg)

### 4.2 树的复杂度举例
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9pwdwenj30rb0bygqt.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9pvdktrj30sk0mfais.jpg)

### 4.3 目标函数推导
![](https://tva1.sinaimg.cn/large/008eGmZEly1goab7ksar1j30j60epn09.jpg)

![](https://tva1.sinaimg.cn/large/008eGmZEly1goab8kn974j30kp02z746.jpg)

![](https://tva1.sinaimg.cn/large/008eGmZEly1goab5rreowj30jo0l7gre.jpg)

## 5. XGBoost的回归树构建方法
### 5.1 计算分裂节点
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yhoisuj30om05r0tn.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yhdi3dj30k505xabf.jpg)

### 5.2 停止分裂条件判断
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yh325uj30on0cvdk6.jpg)


## 6. XGBoost与GDBT的区别
- 区别一:
	- XGBoost生成CART树考虑了树的复杂度， 
	- GDBT未考虑，GDBT在树的剪枝步骤中考虑了树的复杂度。
- 区别二: 
	- XGBoost是拟合上一轮损失函数的二阶导展开，GDBT是拟合上一轮损失函数的一阶导展开，因此，XGBoost的准确性更高，且满足 相同的训练效果，需要的迭代次数更少。
- 区别三: 
	- XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。

## 7. 安装与使用
- macOS安装
	- brew install libomp
	- pip3 install xgboost
- API
	- 参考[链接](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=xgbclassifier#xgboost.XGBClassifier)

```python
from xgboost import XGBClassifier
param_grid = {
    'max_depth': [1,3,5,7,9,15,20],
    'n_estimators': [3,4,5,6,7,10],
    'min_child_weight':[1,2,3,4,5]
}
xgb = XGBClassifier(learning_rate=0.1, n_jobs=-1,
                    nthread=4, subsample=1, colsample_bytree=1, seed=42, min_child_weight=1)
grid_model_xgb = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1) 
grid_model_xgb.fit(x_train, y_train)
```

# 二、lightGBM

## 1. [介绍](https://github.com/Microsoft/LightGBM)
lightGBM是2017年1月，微软在GItHub上开源的一个新的梯度提升框架。
在开源之后，就被别人冠以“速度惊人”、“支持分布式”、“代码清晰易懂”、“占用内存小”等属性。
LightGBM主打的高效并行训练让其性能超越现有其他boosting工具。在Higgs数据集上的试验表明，LightGBM比XGBoost快将近10倍，内存占 用率大约为XGBoost的1/6。

## 2. lightGBM原理
- lightGBM 主要基于以下方面优化，提升整体特特性:
	- 基于Histogram(直方图)的决策树算法
	- Lightgbm的Histogram(直方图)做差加速 3. 带深度限制的Leaf-wise的叶子生长策略
	- 直接支持类别特征
	- 直接支持高效并行
		- 特征并行
		- 数据并行

## 3. 参考文档
[链接](https://www.biaodianfu.com/lightgbm.html)












