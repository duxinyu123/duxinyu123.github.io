---
layout:     post                    # 使用的布局（不需要改）
title:      集成学习进阶  			    # 标题 
subtitle:   XGBoost、lightGBM	    # 副标题
date:       2021-03-05              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---
# 一、XGBoost
## 1. 介绍
- XGBoost(Extreme Gradient Boosting)全名叫极端梯度提升树，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者 用了XGBoost。
- XGBoost在绝大多数的回归和分类问题上表现的十分顶尖

## 2. 最优模型构建方法
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9h774tyj30mj0g0ae3.jpg)

## 3. XGBoost的目标函数推导
### 3.1 目标函数的确定
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9ja159ij30lg05x3zz.jpg)

### 3.2 CART树的介绍
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9jp8ps0j30h70bnmzp.jpg)

## 4. 树的复杂度定义
### 4.1 定义每课树的复杂度
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9miiyikj30hy04ndgs.jpg)

### 4.2 树的复杂度举例
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9pwdwenj30rb0bygqt.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9pvdktrj30sk0mfais.jpg)

### 4.3 目标函数推导
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9t45n10j30t10lcn2k.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9t31cgyj30qq0jvn2a.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9t213yjj30m60pc0z7.jpg)

## 5. XGBoost的回归树构建方法
### 5.1 计算分裂节点
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yhoisuj30om05r0tn.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yhdi3dj30k505xabf.jpg)

### 5.2 停止分裂条件判断
![](https://tva1.sinaimg.cn/large/008eGmZEly1goa9yh325uj30on0cvdk6.jpg)


## 6. XGBoost与GDBT的区别
- 区别一:
	- XGBoost生成CART树考虑了树的复杂度， 
	- GDBT未考虑，GDBT在树的剪枝步骤中考虑了树的复杂度。
- 区别二: 
	- XGBoost是拟合上一轮损失函数的二阶导展开，GDBT是拟合上一轮损失函数的一阶导展开，因此，XGBoost的准确性更高，且满足 相同的训练效果，需要的迭代次数更少。
- 区别三: 
	- XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。

