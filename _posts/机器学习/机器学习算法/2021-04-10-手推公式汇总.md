---
layout:     post                    # 使用的布局（不需要改）
title:      手推公式汇总  			    # 标题 
subtitle:     				# 副标题
date:       2021-04-10              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---
# 一、逻辑回归
## 1. 逻辑回归模型
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtmb05aeqj30ie049dg3.jpg)
## 2. 求对数似然
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtmbltyrtj30lw0cnq4y.jpg)
## 3. 梯度更新
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtmi2mqxwj31240a443t.jpg)

# 二、SVM
> 参考： https://www.pianshen.com/article/15821257925/
 
## 1. 函数间隔和几何间隔
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtndrmkkhj312y0rwk2y.jpg)
## 2. 目标函数的推导
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnedkq6pj313o0p6wnr.jpg)
## 3. 引入拉格朗日函数
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnhy8mv5j30mg0ammzf.jpg)
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnj4njdnj30lh02v74v.jpg)
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnlr4mf0j30lq059t9t.jpg)
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnoxe3tbj30l104dgmf.jpg)
## 4. 证明原始问题与对偶问题的关系(此步可省)
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnr2uomaj30ly0bmtb6.jpg)
## 5. 求最优解
![](https://tva1.sinaimg.cn/large/008i3skNly1gqtnt2rifyj30xu0u0hdt.jpg)

# 三、GBDT

# 四、XGBoost
## 1. 推导过程梳理
![](https://tva1.sinaimg.cn/large/008i3skNly1gquxzd6qupj30q805ogov.jpg)
## 2. 目标函数的构建
![](https://tva1.sinaimg.cn/large/008i3skNly1gquxzygeavj30s30en0z1.jpg)
## 3. 叠加式训练
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy0929stj30s80eo49k.jpg)
## 4. 使用泰勒级数近似目标函数
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy0ixjhaj30so0es4a4.jpg)
## 5. 重新定义一棵树
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy1bk0mnj30ss0evgts.jpg)
## 6. 树的复杂度定义
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy1j20h5j30s60c70x8.jpg)
## 7. 新的目标函数
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy1yige7j30sr0f5n8q.jpg)
## 8. 寻找最好的split
![](https://tva1.sinaimg.cn/large/008i3skNly1gquy25sbs7j30s00c3472.jpg)

# 五、softmax反向传播

# 六、self-attention归一化