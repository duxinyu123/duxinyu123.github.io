---
layout:     post                    # 使用的布局（不需要改）
title:      线性回归   			    # 标题 		  
subtitle:   概念、损失函数、梯度下降、正则化     # 副标题
date:       2021-02-03              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---
# 一、线性回归
## 1. 简介
线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式；
![](https://tva1.sinaimg.cn/large/008eGmZEly1gncys6miqej30l805lmzm.jpg)

## 2. 线性回归的特征与目标的关系分析
- 线性关系
	- 单变量线性关系
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gncyy1wwqpj30q00e9abj.jpg)
	- 多变量线性关系
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gncyy1o7agj30u60hgq8e.jpg)
- 非线性关系
	
	![](https://tva1.sinaimg.cn/large/008eGmZEly1gncyy1iyw1j30g8094ac6.jpg)

## 3. 线性回归api
```python
from sklearn.linear_model import LinearRegression

x = [[80, 86],
[82, 80],
[85, 78],
[90, 90],
[86, 82],
[82, 90],
[78, 80],
[92, 94]]
y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]
# 实例化API

estimator = LinearRegression() 
# 使用fit方法进行训练 

estimator.fit(x,y)
# 回归系数

estimator.coef_

estimator.predict([[100, 80]])
```

# 二. 线性回归的损失和优化
## 1. 损失函数
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnczdjaokgj30ls063dhh.jpg)


## 2. 优化算法
### 1. 正规方程法(公式推导)
> 只是适合样本和特征比较少的情况

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnczgumus7j30ho0gr0u8.jpg)

### 2. 梯度下降法(Gradient Descent)
> 类比为下山的过程, 函数就代表着一座山, 目标就是找到这个函数的最小值, 也就是山底。

#### 1. 梯度的概念
- 在**单变量**的函数中
	- 梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率;
- 在**多变量**函数中
	- 梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的**上升最快**的方向;

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnczt5u366j30m90dajw0.jpg)

#### 2. 梯度下降公式
![](https://tva1.sinaimg.cn/large/008eGmZEly1gncztjxufqj309r04g749.jpg)

- α的含义
	- α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离
	- 既不能太小(效率低)，也不能过大(错过极值点)
- 为什么梯度要乘以一个负号
	- 梯度的方向实际就是函数在此点上升最快的方向
	- 加负号意味着朝着梯度相反的方向前进

#### 3. 方法对比
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnd03rweakj30ng07ytaz.jpg)


