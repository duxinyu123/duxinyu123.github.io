---
layout:     post                    # 使用的布局（不需要改）
title:      决策树   			    # 标题 
subtitle:   熵、信息增益(ID3)、信息增益率(C4.5)、基尼指数(CART)  # 副标题
date:       2021-02-21              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---

# 一、算法简介
决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucfdjc4kj30j40bnwje.jpg)

# 二、熵

## 1. 概念
- 物理学上
	- 熵 Entropy 是“混乱”程度的量度
	- 系统越有序，熵值越低;系统越混乱或者分散，熵值越高。
- 信息理论
	- 当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。
	- "信息熵" (information entropy)是度量样本集合纯度最常用的一种指标

## 2. 信息熵公式
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucmh16h0j30hl021dfz.jpg)

# 三、决策树划分依据
## 1. 信息增益 —— ID3
- 以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。
- 因此可以使用划分前后 集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏
- **信息增益 = entroy(前) - entroy(后)**

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucpdeuiaj30ks0d7mz0.jpg)

### 1.1 信息增益案例
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve0fqzjfj30n00dbjx3.jpg)

通过计算信息增益可以解决这个问题，统计上右表信息

其中Positive为正样本(已流失)，Negative为负样本(未流失)，下面的数值为不同划分下对应的人数。

可得到三个熵:

- **整体熵**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3od9scj30c001u0ss.jpg)

- **性别的信息增益**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3o2y6fj30kj0b6mz4.jpg)

- **活跃度的信息增益**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3oh1idj30m708uabe.jpg)


活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。

在做特征选择或者数据分析的时候，我们应该重点 考察活跃度这个指标。

### 1.2 存在的缺点

- ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息.
- ID3算法只能对描述属性为离散型属性的数据集构造决策树。

## 2. 信息增益率 —— C4.5
### 2.1 解决了什么问题
- 信息增益准则对**可取值数目较多的属性有所偏好**
- 为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.

### 2.2 定义
增益率是用前面的信息增益Gain(D, a)和属性a对应的"固有值"(intrinsic value) [Quinlan,1993J的比值来共同定义的。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvebctolcj30iy06z3z1.jpg)

### 2.3 案例
继续对上述案例进行信息增益率计算

- **计算属性分类信息度量**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveeinxdtj30fm02ldg8.jpg)

- **计算信息增益率**
	
	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveeil6naj30dn030wew.jpg)

活跃度的信息增益率更高一些，所以在构建决策树的时候，优先选择；

通过这种方式，在选取节点的过程中，我们可以降低取值较多的属性的选取偏好。

### 2.4 C4.5的优势
- 1.用信息增益率来选择属性
	- 克服了用信息增益来选择属性时偏向选择值多的属性的不足
- 2.采用了一种后剪枝方法
	- 避免树的高度无节制的增长，避免过度拟合数据
- 3.对于缺失值的处理
	- 处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值
	- 另外一种更复杂的策略是为A的每个可能值赋予一个概率。
- 4. 可以处理连续数值型属性

### 2.5 C4.5的缺点
- 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
- 此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

## 3. 基尼值和基尼指数 —— CART
### 3.1 CART
CART决策树(Classification and Regression Tree) [Breiman et al., 1984] 使用"基尼指数" (Gini index)来选择划分属性, 这是一种著名的决策树学习算法,分类和回归任务都可用;

### 3.2 基尼值Gini和基尼指数Gini_index
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveuzv7fij30kx08gt9y.jpg)

### 3.3 案例分析
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvezhghi7j30nm0ae0tn.jpg)

- 第一次大循环：
	- 根节点的Gini值为:

		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf404krkj30ck021wei.jpg)

	- 当根据是否有房来进行划分时，Gini指数计算过程为
		
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zz918j30d8047q3i.jpg)

	- 若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益

		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zve86j30k50cjgny.jpg)

	- 同理可得年收入Gini
		
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zq3a3j30n60a6gpx.jpg)

- 第二次大循环
	- 接下来，采用同样的方法，分别计算剩下属性，其中根节点的Gini系数为(此时是否拖欠贷款的各有3个records)

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf9tcqjmj30jz0aygom.jpg)

- 经过如上流程，构建的决策树，如下图
	
	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvfa8mtptj30l609q76c.jpg)

## 4. 多变量决策树 —— OC1
同时，无论是ID3,C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征 决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。
在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1。

## 5. 决策树变量的两种类型
- 数字型
	- 变量类型是整数或浮点数，如前面例子中的“年收入”。用“>=”，“>”,“<”或“<=”作为分割条件(排序后，利用已有的分 割情况，可以优化分割算法的时间复杂度)
- 名称型
	- 类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已 婚”或“离婚”，使用“=”来分割。

## 6. 如何评估分割点的好坏?
- 如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。


# 四、剪枝
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnviek6hkbj30m10bp42l.jpg)

## 1. 剪枝的意义
- 剪枝 (pruning)是决策树学习算法对付"过拟合"的主要手段。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvfnt5nzqj30nt0o6afx.jpg)

## 2. 常用的减枝方法
- 决策树剪枝的基本策略有"预剪枝"(pre-pruning)和"后剪枝"(post- pruning) 。
	- 预剪枝
		- 指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将 当前结点标记为叶结点;
	- 后剪枝
		- 则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树 泛化性能提升，则将该子树替换为叶结点。


## 3. 预剪枝


## 4. 后剪枝

