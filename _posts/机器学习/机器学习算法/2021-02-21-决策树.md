---
layout:     post                    # 使用的布局（不需要改）
title:      决策树   			    # 标题 
subtitle:   熵、信息增益(ID3)、信息增益率(C4.5)、基尼指数(CART)  # 副标题
date:       2021-02-21              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---

# 一、算法简介
决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucfdjc4kj30j40bnwje.jpg)

# 二、熵

## 1. 概念
- 物理学上
	- 熵 Entropy 是“混乱”程度的量度
	- 系统越有序，熵值越低;系统越混乱或者分散，熵值越高。
- 信息理论
	- 当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。
	- "信息熵" (information entropy)是度量样本集合纯度最常用的一种指标

## 2. 信息熵公式
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucmh16h0j30hl021dfz.jpg)

# 三、决策树划分依据
## 1. 信息增益 —— ID3
- 以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。
- 因此可以使用划分前后 集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏
- **信息增益 = entroy(前) - entroy(后)**

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnucpdeuiaj30ks0d7mz0.jpg)

### 1.1 信息增益案例
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve0fqzjfj30n00dbjx3.jpg)

通过计算信息增益可以解决这个问题，统计上右表信息

其中Positive为正样本(已流失)，Negative为负样本(未流失)，下面的数值为不同划分下对应的人数。

可得到三个熵:

- **整体熵**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3od9scj30c001u0ss.jpg)

- **性别的信息增益**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3o2y6fj30kj0b6mz4.jpg)

- **活跃度的信息增益**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnve3oh1idj30m708uabe.jpg)


活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。

在做特征选择或者数据分析的时候，我们应该重点 考察活跃度这个指标。

## 2. 信息增益率 —— C4.5
### 2.1 解决了什么问题
- 信息增益准则对**可取值数目较多的属性有所偏好**
- 为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.

### 2.2 定义
增益率是用前面的信息增益Gain(D, a)和属性a对应的"固有值"(intrinsic value) [Quinlan,1993J的比值来共同定义的。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvebctolcj30iy06z3z1.jpg)

### 2.3 案例
继续对上述案例进行信息增益率计算

- **计算属性分裂信息度量**

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveeinxdtj30fm02ldg8.jpg)

- **计算信息增益率**
	
	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveeil6naj30dn030wew.jpg)

活跃度的信息增益率更高一些，所以在构建决策树的时候，优先选择；

通过这种方式，在选取节点的过程中，我们可以降低取值较多的属性的选取偏好。

### 2.4 C4.5的优势
- 1.用信息增益率来选择属性
	- 克服了用信息增益来选择属性时偏向选择值多的属性的不足
- 2.采用了一种后剪枝方法
	- 避免树的高度无节制的增长，避免过度拟合数据
- 3.对于缺失值的处理
	- 处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值
	- 另外一种更复杂的策略是为A的每个可能值赋予一个概率。

## 3. 基尼值和基尼指数 —— CART
### 3.1 CART
CART决策树(Classification and Regression Tree) [Breiman et al., 1984] 使用"基尼指数" (Gini index)来选择划分属性, 这是一种著名的决策树学习算法,分类和回归任务都可用;

### 3.2 基尼值Gini和基尼指数Gini_index
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnveuzv7fij30kx08gt9y.jpg)

### 3.3 案例分析
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvezhghi7j30nm0ae0tn.jpg)

- 第一次大循环：
	- 根节点的Gini值为:

		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf404krkj30ck021wei.jpg)

	- 当根据是否有房来进行划分时，Gini指数计算过程为
		
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zz918j30d8047q3i.jpg)

	- 若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益

		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zve86j30k50cjgny.jpg)

	- 同理可得年收入Gini
		
		![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf3zq3a3j30n60a6gpx.jpg)

- 第二次大循环
	- 接下来，采用同样的方法，分别计算剩下属性，其中根节点的Gini系数为(此时是否拖欠贷款的各有3个records)

	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvf9tcqjmj30jz0aygom.jpg)

- 经过如上流程，构建的决策树，如下图
	
	![](https://tva1.sinaimg.cn/large/008eGmZEly1gnvfa8mtptj30l609q76c.jpg)









