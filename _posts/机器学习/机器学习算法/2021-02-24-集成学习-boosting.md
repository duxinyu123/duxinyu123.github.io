---
layout:     post                    # 使用的布局（不需要改）
title:      集成学习-Boosting  			    # 标题 
subtitle:   Boosting、AdaBoost、GBDT  # 副标题
date:       2021-02-24              # 时间
author:     新宇                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
---

# 一、Boosting
## 1. Boosting介绍
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyctzlcfgj30io08f0xu.jpg)

### 1.1 实现过程
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnycxknjxyj30e80hc439.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnycxkrudoj30e10gl77o.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnycxkw8lgj30dg0k6ag7.jpg)
### 1.2 Boosting 和 Bagging的对比
- 区别一:数据方面
	- Bagging:对数据进行采样训练; 
	- Boosting:根据前一轮学习结果调整数据的重要性。
- 区别二:投票方面 
	- Bagging:所有学习器平权投票; 
	- Boosting:对学习器进行加权投票。
- 区别三:学习顺序 
	- Bagging的学习是并行的，每个学习器没有依赖关系; 
	- Boosting学习是串行，学习有先后顺序。
- 区别四:主要作用 
	- Bagging主要用于提高泛化性能(解决过拟合，也可以说降低方差) 
	- Boosting主要用于提高训练精度 (解决欠拟合，也可以说降低偏差)

# 二. AdaBoost
> Adaptive Boosting,"自适应增强"的缩写

## 1. 构造过程细节
- 步骤一:初始化训练数据权重相等，训练第一个学习器。
- 步骤二:AdaBoost反复学习基本分类器，在每一轮m = 1, 2, ..., M 顺次的执行下列操作:

	- ![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv689smfj30io0avjtb.jpg)

- 步骤三:对m个学习器进行加权投票

	- ![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv685691j30cl039gm6.jpg)

## 2. 关键点剖析
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv680zqij30l90eeqa2.jpg)

## 3. 案例
> 参考李航-《统计学习方法》相关案例

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv9j3qzuj30j50o9wil.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv9j05mhj30ff0h275k.jpg)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnyv9ivcvaj30f10bk75l.jpg)

## 4. API介绍
- from sklearn.ensemble import AdaBoostClassifier
	- [参考api链接](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)

# 三、GBDT
> GBDT 的全称是Gradient Boosting Decision Tree(梯度提升树)
![](https://tva1.sinaimg.cn/large/008eGmZEly1gnywx1br5uj30pg0nraep.jpg)

- 更新强学习器，需要用到参数学习率:learning_rate=0.1，用lr表示
- 为什么要用学习率呢?这是Shrinkage的思想，如果每次都全部加上(学习率为1)很容易一步学到位导致过拟合。
- 这是因为数据简单每棵树长得一样，导致每一颗树的拟合效果一样，而每棵树都只学上一棵树残差的0.1倍，导致这颗树只能拟合剩余0.9了。



